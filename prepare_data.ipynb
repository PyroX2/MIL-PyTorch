{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbe9552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_patcher import ImagePatcher\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import v2\n",
    "import os\n",
    "from typing import Tuple\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f26f56a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/users/scratch1/s189737/collaborative-learning-diabetic-retinopathy/datasets/eyepacs-aptos-messidor-diabetic-retinopathy-original-preprocessed-color-enhancement/train/multiclass\"\n",
    "\n",
    "output_dataset_path = \"data/eyepacs-aptos\"\n",
    "features_output = os.path.join(output_dataset_path, \"features\")\n",
    "labels_output = os.path.join(output_dataset_path, \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "228a3bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/scratch1/s189737/attention/mil/diabetic_retinopathy/venv/lib64/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78987f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(dataset_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecca952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "patcher = ImagePatcher(patch_size=32, empty_thresh=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "227f036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = dataset[1][0]\n",
    "\n",
    "c, h, w = image.shape\n",
    "patcher.get_tiles(h, w)\n",
    "instances, instances_idx, instances_cords = patcher.convert_img_to_bag(image)\n",
    "\n",
    "reconstructed_image = patcher.reconstruct_image_from_patches(instances, instances_idx, (3, 640, 640))\n",
    "\n",
    "reconstructed_image = reconstructed_image.permute(1, 2, 0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e5a37b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MILDataset(Dataset):\n",
    "    def __init__(self, dataset_path: str, image_patcher: ImagePatcher, transform=None) -> None:\n",
    "        super().__init__()\n",
    "        if transform is None:\n",
    "            transform = v2.Compose([\n",
    "                v2.ToTensor()\n",
    "            ])\n",
    "\n",
    "        self.image_patcher = image_patcher\n",
    "        self.img_folder_dataset = ImageFolder(dataset_path, transform=transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_folder_dataset)\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple:\n",
    "        image, label = self.img_folder_dataset[index]\n",
    "        c, h, w = image.shape\n",
    "        self.image_patcher.get_tiles(h, w)\n",
    "        instances, instances_idx, instances_cords = self.image_patcher.convert_img_to_bag(image)\n",
    "        return instances, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "06001503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/scratch1/s189737/attention/mil/diabetic_retinopathy/venv/lib64/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mil_dataset = MILDataset(dataset_path, patcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9bc371d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Aquires important dimensions\n",
    "    batch_size = len(batch)\n",
    "    c, h, w = batch[0][0].shape[1:]\n",
    "    max_bag_length = max([len(x) for x, y in batch])\n",
    "    \n",
    "    # Initializing placeholders for features and labels\n",
    "    features = torch.zeros((batch_size*max_bag_length, c, h, w))\n",
    "    labels = torch.zeros((len(batch)), dtype=torch.long)\n",
    "\n",
    "    # Masking placeholder, mask = 1 for valid instances, 0 for padded instances\n",
    "    masks = torch.zeros((batch_size*max_bag_length))\n",
    "\n",
    "    # Empty image used for padding\n",
    "    pad_image = torch.zeros((1, c, h, w))\n",
    "\n",
    "    for i, (x, y) in enumerate(batch):\n",
    "        n_instances, c, h, w = x.shape\n",
    "\n",
    "        # Set features and labels\n",
    "        features[i*max_bag_length:(i*max_bag_length+n_instances)] = x\n",
    "        features[(i*max_bag_length+n_instances):(i+1)*max_bag_length] = pad_image.expand((max_bag_length-n_instances, c, h, w))\n",
    "\n",
    "        masks[i*max_bag_length:(i*max_bag_length+n_instances)] = 1\n",
    "        labels[i] = y\n",
    "\n",
    "    return features, labels, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9e015518",
   "metadata": {},
   "outputs": [],
   "source": [
    "mil_dataloader = DataLoader(mil_dataset, batch_size=32, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f791ddd5",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1e5aaf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmil.nn import masked_softmax\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "\n",
    "class ABMIL(torch.nn.Module):\n",
    "    def __init__(self, att_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Feature extractor\n",
    "        self.resnet = resnet18(ResNet18_Weights)\n",
    "        emb_dim = self.resnet.fc.in_features\n",
    "\n",
    "        self.resnet.fc = torch.nn.Identity()\n",
    "\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(emb_dim, att_dim)\n",
    "        self.fc2 = torch.nn.Linear(att_dim, 1)\n",
    "\n",
    "        self.classifier = torch.nn.Linear(emb_dim, 1)\n",
    "\n",
    "    def forward(self, X, mask, batch_size, return_att=False):\n",
    "        bag_size = int(X.shape[0] / batch_size)\n",
    "\n",
    "        # Process only instances that are not masked (i.e., valid instances, not padding)\n",
    "        X = self.resnet(X[mask != 0])  # (batch_size * bag_size, emb_dim)\n",
    "\n",
    "        # Put back the processed instances to their original positions, so that the shape is preserved (as if all instances, including padding, were processed)\n",
    "        resnet_output = torch.zeros((batch_size * bag_size, X.shape[1]), device=X.device)\n",
    "        resnet_output[mask != 0] = X\n",
    "        X = resnet_output\n",
    "\n",
    "        # Reshaping to separate bags from batches\n",
    "        X = X.reshape((batch_size, bag_size, -1))  # (batch_size, bag_size, emb_dim)\n",
    "        mask = mask.reshape((batch_size, bag_size))  # (batch_size, bag_size)\n",
    "\n",
    "        H = torch.tanh(self.fc1(X))  # (batch_size, bag_size, att_dim)\n",
    "        att = torch.sigmoid(self.fc2(H))  # (batch_size, bag_size, 1)\n",
    "\n",
    "        att_s = masked_softmax(att, mask)  # (batch_size, bag_size, 1)\n",
    "        # att_s = torch.nn.functional.softmax(att, dim=1)\n",
    "        X = torch.bmm(att_s.transpose(1, 2), X).squeeze(1)  # (batch_size, emb_dim)\n",
    "        y = self.classifier(X).squeeze(1)  # (batch_size,)\n",
    "        if return_att:\n",
    "            return y, att_s\n",
    "        else:\n",
    "            return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f748413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f553bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/scratch1/s189737/attention/mil/diabetic_retinopathy/venv/lib64/python3.11/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/users/scratch1/s189737/attention/mil/diabetic_retinopathy/venv/lib64/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ABMIL(\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (classifier): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ABMIL(att_dim=128)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2c5f3e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, masks = next(iter(mil_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee6c21c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.to(device)\n",
    "masks = masks.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8487e056",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(features, masks, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4471a5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
